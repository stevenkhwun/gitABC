{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'R version is: 4.0.2'"
      ],
      "text/latex": [
       "'R version is: 4.0.2'"
      ],
      "text/markdown": [
       "'R version is: 4.0.2'"
      ],
      "text/plain": [
       "[1] \"R version is: 4.0.2\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paste(\"R version is:\", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Opinions and Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding How Machines Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we extesively use the tm library (http://tm.r-forge.r-project.org/) for R, which easily transforms textual data into numeric matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing tm library, if not yet available\n",
    "if (!(\"tm\" %in% rownames(installed.packages()))) {\n",
    "    install.packages(\"tm\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 <- 'The quick brown fox jumps over the lazy dog.'\n",
    "text_2 <- 'My dog is quick and can jump over fences.'\n",
    "text_3 <- 'Your dog is so lazy that it sleeps all the day.'\n",
    "corpus <- c(text_1, text_2, text_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lacking equiparable functions in R as provided in Python by the Scikit-learn package, we can obtain similar results leveraging tm data structures, such as the <EM>DocumentTermMatrix</EM>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: NLP\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<DocumentTermMatrix (documents: 3, terms: 17)>>\n",
      "Non-/sparse entries: 23/28\n",
      "Sparsity           : 55%\n",
      "Maximal term length: 6\n",
      "Weighting          : term frequency (tf)\n",
      "Sample             :\n",
      "    Terms\n",
      "Docs all and brown can day dog lazy over quick the\n",
      "   1   0   0     1   0   0   1    1    1     1   2\n",
      "   2   0   1     0   1   0   1    0    1     1   0\n",
      "   3   1   0     0   0   1   1    1    0     0   1\n"
     ]
    }
   ],
   "source": [
    "library(tm)\n",
    "corpus <- VCorpus(VectorSource(corpus)) \n",
    "dtm <- DocumentTermMatrix(corpus,            \n",
    "                          control = list(removePunctuation = TRUE,\n",
    "                                         stopwords=FALSE,\n",
    "                                         tolower = TRUE)\n",
    ")\n",
    "inspect(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the dictionary of terms from our matrix, we can use the Term command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'all'</li><li>'and'</li><li>'brown'</li><li>'can'</li><li>'day'</li><li>'dog'</li><li>'fences'</li><li>'fox'</li><li>'jump'</li><li>'jumps'</li><li>'lazy'</li><li>'over'</li><li>'quick'</li><li>'sleeps'</li><li>'that'</li><li>'the'</li><li>'your'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'all'\n",
       "\\item 'and'\n",
       "\\item 'brown'\n",
       "\\item 'can'\n",
       "\\item 'day'\n",
       "\\item 'dog'\n",
       "\\item 'fences'\n",
       "\\item 'fox'\n",
       "\\item 'jump'\n",
       "\\item 'jumps'\n",
       "\\item 'lazy'\n",
       "\\item 'over'\n",
       "\\item 'quick'\n",
       "\\item 'sleeps'\n",
       "\\item 'that'\n",
       "\\item 'the'\n",
       "\\item 'your'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'all'\n",
       "2. 'and'\n",
       "3. 'brown'\n",
       "4. 'can'\n",
       "5. 'day'\n",
       "6. 'dog'\n",
       "7. 'fences'\n",
       "8. 'fox'\n",
       "9. 'jump'\n",
       "10. 'jumps'\n",
       "11. 'lazy'\n",
       "12. 'over'\n",
       "13. 'quick'\n",
       "14. 'sleeps'\n",
       "15. 'that'\n",
       "16. 'the'\n",
       "17. 'your'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"all\"    \"and\"    \"brown\"  \"can\"    \"day\"    \"dog\"    \"fences\" \"fox\"   \n",
       " [9] \"jump\"   \"jumps\"  \"lazy\"   \"over\"   \"quick\"  \"sleeps\" \"that\"   \"the\"   \n",
       "[17] \"your\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Terms(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Enhancing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<DocumentTermMatrix (documents: 4, terms: 21)>>\n",
      "Non-/sparse entries: 29/55\n",
      "Sparsity           : 65%\n",
      "Maximal term length: 6\n",
      "Weighting          : term frequency (tf)\n",
      "Sample             :\n",
      "    Terms\n",
      "Docs all and black brown but dog lazy over quick the\n",
      "   1   0   0     0     1   0   1    1    1     1   2\n",
      "   2   0   1     0     0   0   1    0    1     1   0\n",
      "   3   1   0     0     0   0   1    1    0     0   1\n",
      "   4   0   0     1     1   1   2    0    0     0   0\n"
     ]
    }
   ],
   "source": [
    "text_4 <- 'A black dog just passed by but my dog is brown.'\n",
    "corpus <- c(text_1, text_2, text_3, text_4)\n",
    "corpus <- VCorpus(VectorSource(corpus)) \n",
    "dtm <- DocumentTermMatrix(corpus,            \n",
    "                          control = list(removePunctuation = TRUE,\n",
    "                                         stopwords=FALSE,\n",
    "                                         tolower = TRUE)\n",
    "                         )\n",
    "inspect(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>all</dt><dd>1</dd><dt>and</dt><dd>1</dd><dt>black</dt><dd>1</dd><dt>brown</dt><dd>2</dd><dt>but</dt><dd>1</dd><dt>can</dt><dd>1</dd><dt>day</dt><dd>1</dd><dt>dog</dt><dd>5</dd><dt>fences</dt><dd>1</dd><dt>fox</dt><dd>1</dd><dt>jump</dt><dd>1</dd><dt>jumps</dt><dd>1</dd><dt>just</dt><dd>1</dd><dt>lazy</dt><dd>2</dd><dt>over</dt><dd>2</dd><dt>passed</dt><dd>1</dd><dt>quick</dt><dd>2</dd><dt>sleeps</dt><dd>1</dd><dt>that</dt><dd>1</dd><dt>the</dt><dd>3</dd><dt>your</dt><dd>1</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[all] 1\n",
       "\\item[and] 1\n",
       "\\item[black] 1\n",
       "\\item[brown] 2\n",
       "\\item[but] 1\n",
       "\\item[can] 1\n",
       "\\item[day] 1\n",
       "\\item[dog] 5\n",
       "\\item[fences] 1\n",
       "\\item[fox] 1\n",
       "\\item[jump] 1\n",
       "\\item[jumps] 1\n",
       "\\item[just] 1\n",
       "\\item[lazy] 2\n",
       "\\item[over] 2\n",
       "\\item[passed] 1\n",
       "\\item[quick] 2\n",
       "\\item[sleeps] 1\n",
       "\\item[that] 1\n",
       "\\item[the] 3\n",
       "\\item[your] 1\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "all\n",
       ":   1and\n",
       ":   1black\n",
       ":   1brown\n",
       ":   2but\n",
       ":   1can\n",
       ":   1day\n",
       ":   1dog\n",
       ":   5fences\n",
       ":   1fox\n",
       ":   1jump\n",
       ":   1jumps\n",
       ":   1just\n",
       ":   1lazy\n",
       ":   2over\n",
       ":   2passed\n",
       ":   1quick\n",
       ":   2sleeps\n",
       ":   1that\n",
       ":   1the\n",
       ":   3your\n",
       ":   1\n",
       "\n"
      ],
      "text/plain": [
       "   all    and  black  brown    but    can    day    dog fences    fox   jump \n",
       "     1      1      1      2      1      1      1      5      1      1      1 \n",
       " jumps   just   lazy   over passed  quick sleeps   that    the   your \n",
       "     1      1      2      2      1      2      1      1      3      1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply(as.matrix(dtm), 2, sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<DocumentTermMatrix (documents: 4, terms: 21)>>\n",
      "Non-/sparse entries: 25/59\n",
      "Sparsity           : 70%\n",
      "Maximal term length: 6\n",
      "Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)\n",
      "Sample             :\n",
      "    Terms\n",
      "Docs       and     black     brown       but       can    fences      jump\n",
      "   1 0.0000000 0.0000000 0.1111111 0.0000000 0.0000000 0.0000000 0.0000000\n",
      "   2 0.2857143 0.0000000 0.0000000 0.0000000 0.2857143 0.2857143 0.2857143\n",
      "   3 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n",
      "   4 0.0000000 0.2857143 0.1428571 0.2857143 0.0000000 0.0000000 0.0000000\n",
      "    Terms\n",
      "Docs      just    passed       the\n",
      "   1 0.0000000 0.0000000 0.2222222\n",
      "   2 0.0000000 0.0000000 0.0000000\n",
      "   3 0.0000000 0.0000000 0.1250000\n",
      "   4 0.2857143 0.2857143 0.0000000\n"
     ]
    }
   ],
   "source": [
    "dtm <- DocumentTermMatrix(corpus,\n",
    "           control = list(weighting = function(x) weightTfIdf(x, normalize=TRUE),\n",
    "                          removePunctuation = TRUE,\n",
    "                          stopwords=FALSE,\n",
    "                          tolower = TRUE)\n",
    "                          )\n",
    "\n",
    "\n",
    "inspect(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the tm FAQ (http://tm.r-forge.r-project.org/faq.html#Bigrams), n-grams can be obtained using the ngrams function that can be found in the NLP. In our example we prefer to use the Weka_tokenizers in the RWeka packages because they are faster and more robust, though the usage is quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: RWeka\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (!require(\"RWeka\")) install.packages(\"RWeka\", repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<TermDocumentMatrix (terms: 33, documents: 4)>>\n",
      "Non-/sparse entries: 36/96\n",
      "Sparsity           : 73%\n",
      "Maximal term length: 11\n",
      "Weighting          : term frequency (tf)\n",
      "Sample             :\n",
      "           Docs\n",
      "Terms       1 2 3 4\n",
      "  a black   0 0 0 1\n",
      "  all the   0 0 1 0\n",
      "  and can   0 1 0 0\n",
      "  black dog 0 0 0 1\n",
      "  brown fox 1 0 0 0\n",
      "  but my    0 0 0 1\n",
      "  by but    0 0 0 1\n",
      "  can jump  0 1 0 0\n",
      "  dog is    0 1 1 1\n",
      "  my dog    0 1 0 1\n"
     ]
    }
   ],
   "source": [
    "library(RWeka)\n",
    "\n",
    "BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))\n",
    "\n",
    "dtm <- TermDocumentMatrix(corpus, \n",
    "                          control=list(tokenize=BigramTokenizer,\n",
    "                                       removePunctuation = TRUE,\n",
    "                                       stopwords=FALSE,\n",
    "                                       tolower = TRUE))\n",
    "\n",
    "inspect(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'a black'</li><li>'all the'</li><li>'and can'</li><li>'black dog'</li><li>'brown fox'</li><li>'but my'</li><li>'by but'</li><li>'can jump'</li><li>'dog is'</li><li>'dog just'</li><li>'fox jumps'</li><li>'is brown'</li><li>'is quick'</li><li>'is so'</li><li>'it sleeps'</li><li>'jump over'</li><li>'jumps over'</li><li>'just passed'</li><li>'lazy dog'</li><li>'lazy that'</li><li>'my dog'</li><li>'over fences'</li><li>'over the'</li><li>'passed by'</li><li>'quick and'</li><li>'quick brown'</li><li>'sleeps all'</li><li>'so lazy'</li><li>'that it'</li><li>'the day'</li><li>'the lazy'</li><li>'the quick'</li><li>'your dog'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'a black'\n",
       "\\item 'all the'\n",
       "\\item 'and can'\n",
       "\\item 'black dog'\n",
       "\\item 'brown fox'\n",
       "\\item 'but my'\n",
       "\\item 'by but'\n",
       "\\item 'can jump'\n",
       "\\item 'dog is'\n",
       "\\item 'dog just'\n",
       "\\item 'fox jumps'\n",
       "\\item 'is brown'\n",
       "\\item 'is quick'\n",
       "\\item 'is so'\n",
       "\\item 'it sleeps'\n",
       "\\item 'jump over'\n",
       "\\item 'jumps over'\n",
       "\\item 'just passed'\n",
       "\\item 'lazy dog'\n",
       "\\item 'lazy that'\n",
       "\\item 'my dog'\n",
       "\\item 'over fences'\n",
       "\\item 'over the'\n",
       "\\item 'passed by'\n",
       "\\item 'quick and'\n",
       "\\item 'quick brown'\n",
       "\\item 'sleeps all'\n",
       "\\item 'so lazy'\n",
       "\\item 'that it'\n",
       "\\item 'the day'\n",
       "\\item 'the lazy'\n",
       "\\item 'the quick'\n",
       "\\item 'your dog'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'a black'\n",
       "2. 'all the'\n",
       "3. 'and can'\n",
       "4. 'black dog'\n",
       "5. 'brown fox'\n",
       "6. 'but my'\n",
       "7. 'by but'\n",
       "8. 'can jump'\n",
       "9. 'dog is'\n",
       "10. 'dog just'\n",
       "11. 'fox jumps'\n",
       "12. 'is brown'\n",
       "13. 'is quick'\n",
       "14. 'is so'\n",
       "15. 'it sleeps'\n",
       "16. 'jump over'\n",
       "17. 'jumps over'\n",
       "18. 'just passed'\n",
       "19. 'lazy dog'\n",
       "20. 'lazy that'\n",
       "21. 'my dog'\n",
       "22. 'over fences'\n",
       "23. 'over the'\n",
       "24. 'passed by'\n",
       "25. 'quick and'\n",
       "26. 'quick brown'\n",
       "27. 'sleeps all'\n",
       "28. 'so lazy'\n",
       "29. 'that it'\n",
       "30. 'the day'\n",
       "31. 'the lazy'\n",
       "32. 'the quick'\n",
       "33. 'your dog'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"a black\"     \"all the\"     \"and can\"     \"black dog\"   \"brown fox\"  \n",
       " [6] \"but my\"      \"by but\"      \"can jump\"    \"dog is\"      \"dog just\"   \n",
       "[11] \"fox jumps\"   \"is brown\"    \"is quick\"    \"is so\"       \"it sleeps\"  \n",
       "[16] \"jump over\"   \"jumps over\"  \"just passed\" \"lazy dog\"    \"lazy that\"  \n",
       "[21] \"my dog\"      \"over fences\" \"over the\"    \"passed by\"   \"quick and\"  \n",
       "[26] \"quick brown\" \"sleeps all\"  \"so lazy\"     \"that it\"     \"the day\"    \n",
       "[31] \"the lazy\"    \"the quick\"   \"your dog\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Terms(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (!(\"SnowballC\" %in% rownames(installed.packages()))) {\n",
    "    install.packages(\"SnowballC\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<TermDocumentMatrix (terms: 13, documents: 4)>>\n",
      "Non-/sparse entries: 20/32\n",
      "Sparsity           : 62%\n",
      "Maximal term length: 5\n",
      "Weighting          : term frequency (tf)\n",
      "Sample             :\n",
      "       Docs\n",
      "Terms   1 2 3 4\n",
      "  black 0 0 0 1\n",
      "  brown 1 0 0 1\n",
      "  can   0 1 0 0\n",
      "  day   0 0 1 0\n",
      "  dog   1 1 1 2\n",
      "  fenc  0 1 0 0\n",
      "  fox   1 0 0 0\n",
      "  jump  1 1 0 0\n",
      "  lazi  1 0 1 0\n",
      "  quick 1 1 0 0\n"
     ]
    }
   ],
   "source": [
    "dtm <- TermDocumentMatrix(corpus, \n",
    "                          control=list(removePunctuation = TRUE,\n",
    "                                       stopwords = stopwords(\"english\"), \n",
    "                                       tolower = TRUE, \n",
    "                                       stemming = TRUE\n",
    "                                       ))\n",
    "\n",
    "inspect(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Textual Datasets from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: xml2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (!(\"rvest\" %in% rownames(installed.packages()))) {\n",
    "    install.packages(\"rvest\")\n",
    "}\n",
    "\n",
    "library(rvest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki <- \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "html_data <- read_html(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_tables <- html_data %>% html_table(fill=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_cities <- extracted_tables[[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 5 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>2019rank</th><th scope=col>City</th><th scope=col>State[c]</th><th scope=col>2019estimate</th><th scope=col>2010Census</th><th scope=col>2016 land area</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>New York[d]</td><td>New York  </td><td>8,336,817</td><td>8,175,133</td><td>301.5 sq mi</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2</td><td>Los Angeles</td><td>California</td><td>3,979,576</td><td>3,792,621</td><td>468.7 sq mi</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3</td><td>Chicago    </td><td>Illinois  </td><td>2,693,976</td><td>2,695,598</td><td>227.3 sq mi</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4</td><td>Houston[3] </td><td>Texas     </td><td>2,320,268</td><td>2,100,263</td><td>637.5 sq mi</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5</td><td>Phoenix    </td><td>Arizona   </td><td>1,680,992</td><td>1,445,632</td><td>517.6 sq mi</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 6\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & 2019rank & City & State{[}c{]} & 2019estimate & 2010Census & 2016 land area\\\\\n",
       "  & <int> & <chr> & <chr> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & 1 & New York{[}d{]} & New York   & 8,336,817 & 8,175,133 & 301.5 sq mi\\\\\n",
       "\t2 & 2 & Los Angeles & California & 3,979,576 & 3,792,621 & 468.7 sq mi\\\\\n",
       "\t3 & 3 & Chicago     & Illinois   & 2,693,976 & 2,695,598 & 227.3 sq mi\\\\\n",
       "\t4 & 4 & Houston{[}3{]}  & Texas      & 2,320,268 & 2,100,263 & 637.5 sq mi\\\\\n",
       "\t5 & 5 & Phoenix     & Arizona    & 1,680,992 & 1,445,632 & 517.6 sq mi\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 6\n",
       "\n",
       "| <!--/--> | 2019rank &lt;int&gt; | City &lt;chr&gt; | State[c] &lt;chr&gt; | 2019estimate &lt;chr&gt; | 2010Census &lt;chr&gt; | 2016 land area &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 1 | 1 | New York[d] | New York   | 8,336,817 | 8,175,133 | 301.5 sq mi |\n",
       "| 2 | 2 | Los Angeles | California | 3,979,576 | 3,792,621 | 468.7 sq mi |\n",
       "| 3 | 3 | Chicago     | Illinois   | 2,693,976 | 2,695,598 | 227.3 sq mi |\n",
       "| 4 | 4 | Houston[3]  | Texas      | 2,320,268 | 2,100,263 | 637.5 sq mi |\n",
       "| 5 | 5 | Phoenix     | Arizona    | 1,680,992 | 1,445,632 | 517.6 sq mi |\n",
       "\n"
      ],
      "text/plain": [
       "  2019rank City        State[c]   2019estimate 2010Census 2016 land area\n",
       "1 1        New York[d] New York   8,336,817    8,175,133  301.5 sq mi   \n",
       "2 2        Los Angeles California 3,979,576    3,792,621  468.7 sq mi   \n",
       "3 3        Chicago     Illinois   2,693,976    2,695,598  227.3 sq mi   \n",
       "4 4        Houston[3]  Texas      2,320,268    2,100,263  637.5 sq mi   \n",
       "5 5        Phoenix     Arizona    1,680,992    1,445,632  517.6 sq mi   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_columns <- c(1, 2, 3, 4, 5, 7)\n",
    "table_of_cities[1:5, selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scoring and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing feather library, if not yet available\n",
    "if (!(\"feather\" %in% rownames(installed.packages()))) {\n",
    "    install.packages(\"feather\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing feather library, if not yet available\n",
    "if (!(\"RCurl\" %in% rownames(installed.packages()))) {\n",
    "    install.packages(\"RCurl\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(feather)\n",
    "library(RCurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url <- \"https://github.com/lmassaron/datasets/releases/download/1.0/shakespeare_lines_in_plays.feather\"\n",
    "destfile <- \"shakespeare_lines_in_plays.feather\"\n",
    "download.file(url, destfile, mode =  \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare <- read_feather(destfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tm)\n",
    "\n",
    "if (!(\"irlba\" %in% rownames(installed.packages()))) {\n",
    "    install.packages(\"irlba\")\n",
    "}\n",
    "\n",
    "library(irlba)\n",
    "\n",
    "if (!(\"Matrix\" %in% rownames(installed.packages()))) {\n",
    "    install.packages(\"Matrix\")\n",
    "}\n",
    "\n",
    "library(Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<DocumentTermMatrix (documents: 217, terms: 26675)>>\n",
      "Non-/sparse entries: 195029/5593446\n",
      "Sparsity           : 97%\n",
      "Maximal term length: 37\n",
      "Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)\n",
      "Sample             :\n",
      "     Terms\n",
      "Docs  antipholus      enter     exeunt       exit        king    reenter\n",
      "  1    0.4537249 0.10261048 0.11811845 0.13563933 0.000000000 0.01860786\n",
      "  109  0.0000000 0.08278573 0.12811678 0.12923548 0.004722556 0.04986377\n",
      "  121  0.0000000 0.11077519 0.15882713 0.11978962 0.000000000 0.00000000\n",
      "  133  0.0000000 0.10086558 0.18944237 0.10877148 0.000000000 0.04476587\n",
      "  139  0.0000000 0.08862015 0.11294374 0.18157585 0.000000000 0.07265336\n",
      "  169  0.0000000 0.08422998 0.12686652 0.17258073 0.000000000 0.09147436\n",
      "  176  0.0000000 0.06667025 0.09150534 0.09807338 0.000000000 0.05766139\n",
      "  200  0.0000000 0.10553582 0.12591700 0.22696981 0.000000000 0.08836219\n",
      "  31   0.0000000 0.12389330 0.16718645 0.10618471 0.000000000 0.01365665\n",
      "  7    0.0000000 0.08357300 0.10651129 0.21488266 0.000000000 0.09672784\n",
      "     Terms\n",
      "Docs        scene  syracuse thou will\n",
      "  1   0.063886356 0.6151981    0    0\n",
      "  109 0.005187795 0.0000000    0    0\n",
      "  121 0.076937887 0.0000000    0    0\n",
      "  133 0.074518619 0.0000000    0    0\n",
      "  139 0.048592349 0.0000000    0    0\n",
      "  169 0.047024854 0.0000000    0    0\n",
      "  176 0.040493625 0.0000000    0    0\n",
      "  200 0.070918564 0.0000000    0    0\n",
      "  31  0.093774710 0.0000000    0    0\n",
      "  7   0.038816374 0.0000000    0    0\n"
     ]
    }
   ],
   "source": [
    "corpus <- VCorpus(VectorSource(shakespeare$lines)) \n",
    "dtm <- DocumentTermMatrix(corpus,\n",
    "           control = list(weighting=function(x) weightTfIdf(x, normalize=TRUE),\n",
    "                          stopwords=stopwords(\"english\"), \n",
    "                          removePunctuation=TRUE,\n",
    "                          removeNumbers=TRUE,\n",
    "                          tolower=TRUE,\n",
    "                          wordLengths=c(4,Inf))\n",
    "                          )\n",
    "\n",
    "inspect(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms <- dtm$dimnames[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix <- sparseMatrix(i=dtm$i, j=dtm$j, x=dtm$v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "res <- irlba(sparse_matrix, n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics <- res$v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"topic 1 | top words: exit exeunt enter scene reenter\"\n",
      "[1] \"topic 2 | top words: syracuse antipholus dromio ephesus luciana\"\n",
      "[1] \"topic 3 | top words: maria toby belch fabian viola\"\n",
      "[1] \"topic 4 | top words: iago othello desdemona cassio emilia\"\n",
      "[1] \"topic 5 | top words: orlando leonato rosalind pedro celia\"\n",
      "[1] \"topic 6 | top words: iago othello cassio desdemona emilia\"\n",
      "[1] \"topic 7 | top words: ariel ferdinand caliban costard biondello\"\n",
      "[1] \"topic 8 | top words: biondello bianca katharina lucentio ariel\"\n",
      "[1] \"topic 9 | top words: ariel costard moth boyet biron\"\n",
      "[1] \"topic 10 | top words: proteus silvia thurio salarino provost\"\n"
     ]
    }
   ],
   "source": [
    "top_words = 5\n",
    "for (topic in 1:n_topics) {\n",
    "    print(paste(\"topic\", topic, \n",
    "                \"| top words:\", paste(terms[order(abs(topics[,topic]), decreasing=T)[1:top_words]], collapse=\" \")\n",
    "               )\n",
    "         )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing reviews from e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(feather)\n",
    "library(RCurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url <- \"https://github.com/lmassaron/datasets/releases/download/1.0/imdb_50k.feather\"\n",
    "destfile <- \"imdb_50k.feather\"\n",
    "download.file(url, destfile, mode =  \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews <- read_feather(destfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in normalizePath(path.expand(path), winslash, mustWork):\n",
      "\"path[1]=\"C:\\Users\\Luca\\anaconda3\\envs\\ml4d/python.exe\": Impossibile trovare il file specificato\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "python:         C:/Users/Luca/anaconda3/python.exe\n",
       "libpython:      C:/Users/Luca/anaconda3/python37.dll\n",
       "pythonhome:     C:/Users/Luca/anaconda3\n",
       "version:        3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
       "Architecture:   64bit\n",
       "numpy:          C:/Users/Luca/anaconda3/Lib/site-packages/numpy\n",
       "numpy_version:  1.18.1\n",
       "\n",
       "python versions found: \n",
       " C:/Users/Luca/anaconda3/python.exe\n",
       " C:/Users/Luca/anaconda3/envs/algo4dummies/python.exe\n",
       " C:/Users/Luca/anaconda3/envs/dl4dummies/python.exe\n",
       " C:/Users/Luca/anaconda3/envs/ml4dit/python.exe\n",
       " C:/Users/Luca/anaconda3/envs/p4ds4d/python.exe"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On Windows, first install Rtools following instructions at: https://cran.r-project.org/bin/windows/Rtools/\n",
    "\n",
    "if (!(\"keras\" %in% rownames(installed.packages()))) {\n",
    "    install.packages(\"backports\", type='binary')\n",
    "    install.packages(\"devtools\")\n",
    "    devtools::install_github(\"rstudio/keras\", force=TRUE)\n",
    "    reticulate::py_config()\n",
    "} else {\n",
    "    reticulate::py_config()\n",
    "}\n",
    "\n",
    "# If necessary, please download and install Rtools 3.5 from http://cran.r-project.org/bin/windows/Rtools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras) # installation instructions are on the notebook from chapter 14th\n",
    "\n",
    "num_words=10000\n",
    "tokenizer <- text_tokenizer()\n",
    "tokenizer %>% fit_text_tokenizer(reviews$review[1:30000])\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "X <- pad_sequences(texts_to_sequences(tokenizer, reviews$review[1:30000]), maxlen=max_len)\n",
    "y <- reviews$sentiment[1:30000]\n",
    "\n",
    "Xv <- pad_sequences(texts_to_sequences(tokenizer, reviews$review[30001:40000]), maxlen=max_len)\n",
    "yv <- reviews$sentiment[30001:40000]\n",
    "\n",
    "Xt <- pad_sequences(texts_to_sequences(tokenizer, reviews$review[40001:50000]), maxlen=max_len)\n",
    "yt <- reviews$sentiment[40001:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "Model: \"sequential\"\n",
       "________________________________________________________________________________\n",
       "Layer (type)                        Output Shape                    Param #     \n",
       "================================================================================\n",
       "embedding (Embedding)               (None, 256, 8)                  787184      \n",
       "________________________________________________________________________________\n",
       "flatten (Flatten)                   (None, 2048)                    0           \n",
       "________________________________________________________________________________\n",
       "dropout (Dropout)                   (None, 2048)                    0           \n",
       "________________________________________________________________________________\n",
       "dense (Dense)                       (None, 1)                       2049        \n",
       "================================================================================\n",
       "Total params: 789,233\n",
       "Trainable params: 789,233\n",
       "Non-trainable params: 0\n",
       "________________________________________________________________________________\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_size <- 8\n",
    "\n",
    "# Add layers to the model\n",
    "\n",
    "model <- keras_model_sequential() %>% \n",
    "  layer_embedding(\n",
    "      input_dim=max(X, Xv, Xt) + 1, \n",
    "      output_dim=embedding_size, \n",
    "      input_length=max_len) %>% \n",
    "  layer_flatten() %>%\n",
    "  layer_dropout(rate=0.2) %>%\n",
    "  layer_dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "# Compile the model\n",
    "model %>% compile(\n",
    "  loss = loss_binary_crossentropy,\n",
    "  optimizer = optimizer_adam(),\n",
    "  metrics=c('acc')\n",
    ")\n",
    "\n",
    "# Summary of the model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model's training parameters\n",
    "epochs=2\n",
    "batch_size=16\n",
    "\n",
    "# Training the model\n",
    "history <- model %>% fit(\n",
    "    X, y,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data=list(Xv, yv),\n",
    "    verbose=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2561352 \n",
      "Validation accuracy: 0.897 \n"
     ]
    }
   ],
   "source": [
    "# Computing validation metrics\n",
    "scores <- model %>% evaluate(Xv, yv, verbose=0)\n",
    "\n",
    "# Printing the scores\n",
    "cat('Validation loss:', scores[[1]], '\\n')\n",
    "cat('Validation accuracy:', scores[[2]], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2496146 \n",
      "Test accuracy: 0.8988 \n"
     ]
    }
   ],
   "source": [
    "# Computing test metrics\n",
    "scores <- model %>% evaluate(Xt, yt, verbose=0)\n",
    "\n",
    "# Printing the scores\n",
    "cat('Test loss:', scores[[1]], '\\n')\n",
    "cat('Test accuracy:', scores[[2]], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba <- model %>% predict(Xt)\n",
    "\n",
    "# Transforming probabilities into a binary variable\n",
    "# using the 0.5 probability threshold\n",
    "preds <- as.numeric(proba>=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
